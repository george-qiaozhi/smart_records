# Collect SMART data
This memo shows how to collect SMART data of disks on our server. The data collected will be correlated with workload information, I/O stat, and other environment attributes, for future analysis of our **disk failure prediction** project.
###### system configurations:
- Server s209: 
  - From internal/lab VLAN:  `ssh zfs@192.168.1.22`
  - Seagate Barracuda 2TB HDD 3.5"FF x 4
  - Samsung 840 evo boot drive
  - SATA channel
- Server s211:
  - From internal/lab VLAN:  `ssh zfs@192.168.1.23`
  - Intel SSD DC 240GB 2.5"FF x 4
  - HDD as boot drive
  - SATA channel


## prerequisites

##### smartctl 
- run as root

```
apt install smartmontools
smartctl -A /dev/sdb
```

##### cron background
- edit cron job list using preferred user account
  - for smartctl, root is a must have, so `su -` or just `su`
- `crontab -e` to edit the cron job list
```
SHELL=/bin/bash
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games

@hourly /home/zfs/SMART_cron/checkSMARThourly_v2.sh
```
- configure the PATH appropriately or use program's full path when invoke.
- available flags are @hourly @reboot @ monthly etc.
- OR, use `* * * * * [user] </script/path>` to specify cron job **every minute**

##### shell script
- currently in `/home/zfs/SMART_cron/check_smart_cron_per_minute.sh`
- using timestamp as file name: `date +%Y-%m-%d-%H-%M`
- Each disk have a dedicated directory for SMART data

## data collection
Currently we collect data for SSD stress test. The stress test use `dd if=xx of=yy oflag=dsync` to read input and write to output dir. ZFS is used to manage each SSD. SSD#0 is a single disk zpool that has input file `10G.dat`, which filled with random number generated by `openssl`. SSD#1 and SSD#2 is bundled by zfs via `zpool create -f outpool sdc sdd`, where data writes to, then removed. SSD#3 is another single disk pool, logs smart record every minutes. 

- From `crontab -e`, initiate cron job to record smart data every minute.
- once error detected, email notifications.

## results
Comming soon..

# SSD Stress Test
This test is trying to constantly write data to SSD and see how long it will fail. The experiment is inspired by a blog from 2015: [The SSD Endurance Experiment: They're all dead
](https://techreport.com/review/27909/the-ssd-endurance-experiment-theyre-all-dead)
We try to answer following questions:
- How is the SSD IO performance while the writes piles up?
- How is the reliability attributes changes .... ?
- What is the amount of write it will take to crash a SSD?
- and etc., while we continue our testing.

After a quick online research, we know that:
- the amount of data write to kill a disk >>> the manufacture claimed amount. SSD manufacture usually guarantees about 50~200TB of total write to flash chips, but some experiment found that it usually takes 10x more data write (at PB level) to kill a SSD.

Our plan:
- use `dd` to stream random data to SSD. log the IO performance that reported by `dd`, as well as the time to complete a write job.
- use `cron` to collect SMART data hourly, result save to `/dev/sde`
- as the output dir, `/dev/sdc` and `/dev/sdd` is in a zfs managed RAID-0 group. zfs will ensure the even wear between two SSDs.
- `/dev/sdb` is the input file dir. contains 1G & 20G random file generated by `openssl`.
- `/dev/sdf` is the idle disk that used as compare group. Ideally, write wear to SSD will accelerate the failure.
- all the log saved on `/dev/sde` will be synced daily to boot drive `/dev/sda` via `rsync` + `cron`.

## results

regularly check smart record for /dev/sdc or /dev/sdd, look for SMART attribute #233: media wearout indicator. The original value is 100, now 1 month later its 91. No other SMART record shows significantly change.

Comming soon..


